
# Transformer from Scratch using PyTorch

This project is an **end-to-end implementation of the Transformer architecture** from the seminal paper *"Attention is All You Need"*, built entirely from scratch using PyTorch.

It covers the fundamental components of a Transformer, such as:
- **Positional Encoding**
- **Scaled Dot-Product Attention**
- **Multi-Head Attention**
- **Encoder-Decoder Architecture**
- **Feedforward Neural Networks**
- **Masking for Decoders**

The project aims to deepen the understanding of how Transformers work internally by coding every component manually, without relying on pre-built PyTorch modules.

---

## 🚀 Project Highlights

- **Complete Transformer Architecture**: From embedding layers to the final output layer, including masking and multi-head attention.
- **Educational Focus**: The code follows a clear step-by-step structure for learning purposes, based on the famous *"Attention is All You Need"* paper.
- **Code-First Approach**: While theoretical resources are referenced, the focus is on hands-on coding to understand the architecture.

---

## 🧠 Learning Objectives

- Understand the inner workings of the Transformer model.
- Learn how to build each module of the Transformer, such as self-attention, positional encoding, and multi-head attention.
- Apply PyTorch to implement complex architectures without relying on pre-made abstractions.

---

## 🛠️ Technologies Used

- **Python 3.8+**
- **PyTorch**
- **Jupyter Notebooks**

---

## 📂 Project Structure

├── 1_transformer_from_scratch_using_pytorch.ipynb # Main notebook with the complete implementation
├── README.md
└── .gitignore


---

## 📌 Notes

- The code currently implements a full feed-forward pass of the Transformer model.
- Future updates may include training loops, loss functions, and more advanced features.


---

## 🙌 Credits

This project is inspired by [Rahul Kumar's work](https://github.com/mynamerahulkumar) and adapted for personal learning and educational purposes.

---

